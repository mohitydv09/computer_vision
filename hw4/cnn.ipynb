{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# import main_functions as main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16928b580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Add for Visualization.\n",
    "mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.imshow(mnist_train['im_train'][:, 0].reshape((14, 14), order='F'), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch(im_train, label_train, batch_size):\n",
    "    # TO DO\n",
    "    ## One hot encode the labels.\n",
    "    label_train_encoded = np.eye(10)[label_train[0]].T\n",
    "\n",
    "    ## Get shuffled indices.\n",
    "    np.random.seed(39)\n",
    "    random_indices = np.random.permutation(label_train.shape[1])\n",
    "\n",
    "    ## Get the Mini Batches, numpy automatically takes care of the last batch\n",
    "    mini_batch_x = [im_train[:,random_indices[i*batch_size:(i+1)*batch_size]] for i in range(math.ceil(im_train.shape[1]/batch_size))]\n",
    "    mini_batch_y = [label_train_encoded[:,random_indices[i*batch_size:(i+1)*batch_size]] for i in range(math.ceil(im_train.shape[1]/batch_size))]\n",
    "    \n",
    "    return mini_batch_x, mini_batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x, w, b):\n",
    "    # TO DO\n",
    "    y = w@x + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dl_dy, x, w, b, y):\n",
    "    # TO DO\n",
    "    dl_dw = dl_dy @ x.T\n",
    "    dl_db = dl_dy\n",
    "    dl_dx = w.T @ dl_dy\n",
    "    \n",
    "    return dl_dx, dl_dw, dl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_euclidean(y_tilde, y):\n",
    "    # TO DO\n",
    "    l = np.sum(np.square(y_tilde - y))\n",
    "    dl_dy = 2*(y_tilde - y)\n",
    "    \n",
    "    return l, dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy_softmax(x, y):\n",
    "    # TO DO\n",
    "\n",
    "    ## Apply Softmax layer.\n",
    "    y_telda = np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "    # Get the solfmax loss.\n",
    "    l = -np.sum(y*np.log(y_telda))\n",
    "\n",
    "    dl_dy = y_telda - y\n",
    "\n",
    "    return l, dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # TO DO\n",
    "    x[x < 0] = 0\n",
    "    y = x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    dl_dx = np.where(x > 0, dl_dy, 0)\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(image, kernal_size, stride):\n",
    "    ## Set the number of patches as num of columns.\n",
    "    num_cols = ((image.shape[0]-kernal_size)//stride +1)*((image.shape[1]-kernal_size)//stride + 1)\n",
    "\n",
    "    col_image = np.empty((kernal_size*kernal_size, num_cols))\n",
    "    col_index = 0\n",
    "    for col in range(0, image.shape[1]-kernal_size+1, stride):\n",
    "        for row in range(0, image.shape[0]-kernal_size+1, stride):\n",
    "            patch = image[row:row+kernal_size, col:col+kernal_size]\n",
    "            col_image[:,col_index] = patch.flatten(order='F')\n",
    "            col_index+=1\n",
    "    return col_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, w_conv, b_conv):\n",
    "    # TO DO\n",
    "    ## Pad the input image.\n",
    "    print(\"Shape of x : \", x.shape)\n",
    "    print(\"Shape of w_conv : \", w_conv.shape)\n",
    "    print(\"Shape of b_conv : \", b_conv.shape)\n",
    "\n",
    "    ## im2col Implementation.\n",
    "    ## Create the col_image.\n",
    "    kernal_size = 3\n",
    "    stride = 1\n",
    "    col_image = im2col(x, kernal_size, stride)\n",
    "\n",
    "    ## Create flattned weights.\n",
    "    # w1 = w_conv[:,:,0].flatten(order='F')[::-1]\n",
    "    # w2 = w_conv[:,:,1].flatten(order='F')[::-1]\n",
    "    # w3 = w_conv[:,:,2].flatten(order='F')[::-1]\n",
    "\n",
    "    w = np.reshape(w_conv, (-1,3), order='F')\n",
    "    w = np.flip(w, axis=1)\n",
    "\n",
    "    y = (col_image.T) @ (w.T)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dl_dy, x, w_conv, b_conv, y):\n",
    "    # TO DO\n",
    "    dy_dw = x.T\n",
    "    dl_dw = dl_dy @ dy_dw\n",
    "    dl_db = dl_dy\n",
    "    return dl_dw, dl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2x2(x):\n",
    "    # TO DO\n",
    "    stride = 2\n",
    "    y = np.zeros((x.shape[0]//2, x.shape[1]//2, x.shape[2]))\n",
    "    for channel in range(x.shape[2]):\n",
    "        for col in range(0,x.shape[1],stride):\n",
    "            for row in range(0,x.shape[0],stride):\n",
    "                y[row//2, col//2, channel] = np.max(x[row:row+2, col:col+2])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2x2_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    stride = 2\n",
    "    indices = np.zeros((x.shape[0]//2, x.shape[1]//2, x.shape[2]))\n",
    "    for channel in range(x.shape[2]):\n",
    "        for col in range(0,x.shape[1],stride):\n",
    "            for row in range(0,x.shape[0],stride):\n",
    "                indices[row//2, col//2, channel] = np.argmax(x[row:row+2, col:col+2])\n",
    "    dl_dx = np.zeros_like(x)\n",
    "    dl_dx[indices] = dl_dy\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening(x):\n",
    "    # TO DO\n",
    "    y = x.flatten(order='F')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    dl_dx = dl_dy.reshape(x.shape, order='F')\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_slp_linear(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 0.1\n",
    "    decay_rate = 0.5\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    w = np.random.randn(output_size,input_size)\n",
    "    b = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10000):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw_batch = np.zeros_like(w)\n",
    "        dl_db_batch = np.zeros_like(b)\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            y_tilde = fc(x, w, b)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy = loss_euclidean(y_tilde, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients.\n",
    "            dl_dx, dl_dw, dl_db = fc_backward(dl_dy, x, w, b, y)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_batch += dl_dw\n",
    "            dl_db_batch += dl_db\n",
    "            loss_cum += loss\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w -= (learning_rate/len(mini_batch_x[0]))*dl_dw_batch\n",
    "        b -= (learning_rate/len(mini_batch_x[0]))*dl_db_batch\n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_slp(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 5\n",
    "    decay_rate = 0.7\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    w = np.random.randn(output_size,input_size)\n",
    "    b = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(1001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw_batch = np.zeros_like(w)\n",
    "        dl_db_batch = np.zeros_like(b)\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            y_tilde = fc(x, w, b)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy = loss_cross_entropy_softmax(y_tilde, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients.\n",
    "            dl_dx, dl_dw, dl_db = fc_backward(dl_dy, x, w, b, y)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_batch += dl_dw\n",
    "            dl_db_batch += dl_db\n",
    "            loss_cum += loss\n",
    "        #     break\n",
    "        # break\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w -= (learning_rate/len(mini_batch_x[0]))*dl_dw_batch\n",
    "        b -= (learning_rate/len(mini_batch_x[0]))*dl_db_batch\n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 5\n",
    "    decay_rate = 0.99\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    hidden_size = 30\n",
    "    w1 = np.random.randn(hidden_size,input_size)\n",
    "    b1 = np.random.randn(hidden_size,1)\n",
    "    w2 = np.random.randn(output_size,hidden_size)\n",
    "    b2 = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw1_batch = np.zeros_like(w1)\n",
    "        dl_db1_batch = np.zeros_like(b1)\n",
    "        dl_dw2_batch = np.zeros_like(w2)\n",
    "        dl_db2_batch = np.zeros_like(b2)\n",
    "\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass hidden Layer.\n",
    "            y_out_hidden = fc(x, w1, b1)   ## Output will be a 30 size column matrix\n",
    "            ## Apply ReLU on this.\n",
    "            y_out_relu = relu(y_out_hidden)\n",
    "            ## Forward pass output layer\n",
    "            y_out_last = fc(y_out_relu, w2, b2)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy_last = loss_cross_entropy_softmax(y_out_last, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients last layer.\n",
    "            dl_dx_hidden, dl_dw2, dl_db2 = fc_backward(dl_dy_last, y_out_hidden, w2, b2, y_out_last)\n",
    "            ## Computer Gradients ReLU.\n",
    "            dl_dx_relu = relu_backward(dl_dx_hidden, y_out_hidden, y_out_relu)\n",
    "            ## Compute Gradients First Layer.\n",
    "            dl_dx, dl_dw1, dl_db1 = fc_backward(dl_dx_relu, x, w1, b1, y_out_hidden)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw2_batch += dl_dw2\n",
    "            dl_db2_batch += dl_db2\n",
    "            dl_dw1_batch += dl_dw1\n",
    "            dl_db1_batch += dl_db1\n",
    "            loss_cum += loss\n",
    "        #     break\n",
    "        # break\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w2 -= (learning_rate/len(mini_batch_x[0]))*dl_dw2_batch\n",
    "        b2 -= (learning_rate/len(mini_batch_x[0]))*dl_db2_batch\n",
    "        w1 -= (learning_rate/len(mini_batch_x[0]))*dl_dw1_batch\n",
    "        b1 -= (learning_rate/len(mini_batch_x[0]))*dl_db1_batch   \n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.35698006 0.43682778 0.25812753]]\n",
      "\n",
      "  [[0.66634086 0.61340343 0.38443994]]\n",
      "\n",
      "  [[0.02934244 0.79058592 0.9315521 ]]]\n",
      "\n",
      "\n",
      " [[[0.1345164  0.90604917 0.64496077]]\n",
      "\n",
      "  [[0.90896087 0.39418791 0.3060826 ]]\n",
      "\n",
      "  [[0.71839668 0.75055378 0.30769678]]]\n",
      "\n",
      "\n",
      " [[[0.7101151  0.4600551  0.23210725]]\n",
      "\n",
      "  [[0.86913786 0.60793247 0.88722191]]\n",
      "\n",
      "  [[0.30602088 0.69316721 0.07468357]]]]\n",
      "[[[0.1345164  0.90604917 0.64496077]]\n",
      "\n",
      " [[0.90896087 0.39418791 0.3060826 ]]\n",
      "\n",
      " [[0.71839668 0.75055378 0.30769678]]]\n"
     ]
    }
   ],
   "source": [
    "train_w = np.random.rand(3,3,1,3)\n",
    "print(train_w)\n",
    "print(train_w[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 0.01\n",
    "    decay_rate = 0.7\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    kernal_size = 3\n",
    "    channels_input = 1\n",
    "    channels_output = 3\n",
    "    flattened_size = 147\n",
    "    output_size = 10\n",
    "    w_conv = np.random.randn(kernal_size, kernal_size, channels_input, channels_output)\n",
    "    b_conv = np.random.randn(channels_output,1)\n",
    "    w_fc = np.random.randn(output_size,flattened_size)\n",
    "    b_fc = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "\n",
    "        dl_dw_conv_batch = np.zeros_like(w_conv)\n",
    "        dl_db_conv_batch = np.zeros_like(b_conv)\n",
    "        dl_dw_fc_batch = np.zeros_like(w_fc)\n",
    "        dl_db_fc_batch = np.zeros_like(b_fc)\n",
    "\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape((14,14), order=\"F\") ## Revert back to being a image.\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            y_conv = conv(x, w_conv, b_conv)   ## Output will be a 30 size column matrix\n",
    "            y_relu = relu(y_conv)\n",
    "            y_maxpool = pool2x2(y_relu)\n",
    "            y_flatten = flattening(y_maxpool)\n",
    "            y_fc = fc(y_flatten, w_fc, b_fc)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy_softmax = loss_cross_entropy_softmax(y_fc, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients in reverse.\n",
    "            dl_dx_flatten, dl_dw_fc, dl_db_fc = fc_backward(dl_dy_softmax, y_flatten, w_fc, b_fc, y_fc)\n",
    "            dl_dx_maxpool = flattening_backward(dl_dx_flatten, y_maxpool, y_flatten)\n",
    "            dl_dx_relu = pool2x2_backward(dl_dx_maxpool, y_relu, y_maxpool)\n",
    "            dl_dx_conv = relu_backward(dl_dx_relu, y_relu, y_maxpool)\n",
    "            dl_dw_conv, dl_db_conv = conv_backward(dl_dx_conv, x, w_conv, b_conv, y_conv)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_conv_batch += dl_dw_conv\n",
    "            dl_db_conv_batch += dl_db_conv\n",
    "            dl_dw_fc_batch += dl_dw_fc\n",
    "            dl_db_fc_batch += dl_db_fc\n",
    "            loss_cum += loss\n",
    "            break\n",
    "        break\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w_conv -= (learning_rate/len(mini_batch_x[0]))*dl_dw_conv_batch\n",
    "        b_conv -= (learning_rate/len(mini_batch_x[0]))*dl_db_conv_batch\n",
    "        w_fc -= (learning_rate/len(mini_batch_x[0]))*dl_dw_fc_batch\n",
    "        b_fc -= (learning_rate/len(mini_batch_x[0]))*dl_db_fc_batch   \n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w_conv, b_conv, w_fc, b_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number is :  0  Loss is :  0\n",
      "Shape of x :  (14, 14)\n",
      "Shape of w_conv :  (3, 3, 1, 3)\n",
      "Shape of b_conv :  (3, 1)\n",
      "Shape of col image :  (9, 144)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# main_slp_linear()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# main_slp()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# main_mlp()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 105\u001b[0m, in \u001b[0;36mmain_cnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m im_train, im_test \u001b[38;5;241m=\u001b[39m im_train \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m, im_test \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    104\u001b[0m mini_batch_x, mini_batch_y \u001b[38;5;241m=\u001b[39m get_mini_batch(im_train, label_train, batch_size)\n\u001b[0;32m--> 105\u001b[0m w_conv, b_conv, w_fc, b_fc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmini_batch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m sio\u001b[38;5;241m.\u001b[39msavemat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn.mat\u001b[39m\u001b[38;5;124m'\u001b[39m, mdict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_conv\u001b[39m\u001b[38;5;124m'\u001b[39m: w_conv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_conv\u001b[39m\u001b[38;5;124m'\u001b[39m: b_conv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_fc\u001b[39m\u001b[38;5;124m'\u001b[39m: w_fc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_fc\u001b[39m\u001b[38;5;124m'\u001b[39m: b_fc})\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# could use following two lines to replace above two lines if only want to check results\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# data = sio.loadmat('cnn.mat')\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# w_conv, b_conv, w_fc, b_fc = data['w_conv'], data['b_conv'], data['w_fc'], data['b_fc']\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 35\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[0;34m(mini_batch_x, mini_batch_y)\u001b[0m\n\u001b[1;32m     32\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m## Forward pass.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m y_conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_conv\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m## Output will be a 30 size column matrix\u001b[39;00m\n\u001b[1;32m     36\u001b[0m y_relu \u001b[38;5;241m=\u001b[39m relu(y_conv)\n\u001b[1;32m     37\u001b[0m y_maxpool \u001b[38;5;241m=\u001b[39m pool2x2(y_relu)\n",
      "Cell \u001b[0;32mIn[40], line 22\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(x, w_conv, b_conv)\u001b[0m\n\u001b[1;32m     19\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(w_conv, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflip(w, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mcol_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 9)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # main_slp_linear()\n",
    "    # main_slp()\n",
    "    # main_mlp()\n",
    "    main_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from cnn import get_mini_batch, fc, relu, conv, pool2x2, flattening\n",
    "# from cnn import train_slp_linear, train_slp, train_mlp, train_cnn\n",
    "\n",
    "\n",
    "def main_slp_linear():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w, b = train_slp_linear(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('slp_linear.mat', mdict={'w': w, 'b': b})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        y = fc(x, w, b)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Single-layer Linear Perceptron Confusion Matrix')\n",
    "\n",
    "def main_slp():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w, b = train_slp(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('slp.mat', mdict={'w': w, 'b': b})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        y = fc(x, w, b)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Single-layer Perceptron Confusion Matrix')\n",
    "\n",
    "def main_mlp():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w1, b1, w2, b2 = train_mlp(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('mlp.mat', mdict={'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        pred1 = fc(x, w1, b1)\n",
    "        pred2 = relu(pred1)\n",
    "        y = fc(pred2, w2, b2)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Multi-layer Perceptron Confusion Matrix')\n",
    "\n",
    "def main_cnn():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w_conv, b_conv, w_fc, b_fc = train_cnn(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('cnn.mat', mdict={'w_conv': w_conv, 'b_conv': b_conv, 'w_fc': w_fc, 'b_fc': b_fc})\n",
    "    # could use following two lines to replace above two lines if only want to check results\n",
    "    # data = sio.loadmat('cnn.mat')\n",
    "    # w_conv, b_conv, w_fc, b_fc = data['w_conv'], data['b_conv'], data['w_fc'], data['b_fc']\n",
    "    \n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]].reshape((14, 14, 1), order='F')\n",
    "        pred1 = conv(x, w_conv, b_conv)  # (14, 14, 3)\n",
    "        pred2 = relu(pred1)  # (14, 14, 3)\n",
    "        pred3 = pool2x2(pred2)  # (7, 7, 3)\n",
    "        pred4 = flattening(pred3)  # (147, 1)\n",
    "        y = fc(pred4, w_fc, b_fc)  # (10, 1)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'CNN Confusion Matrix')\n",
    "\n",
    "def visualize_confusion_matrix(confusion, accuracy, label_classes, name):\n",
    "    plt.title(\"{}, accuracy = {:.3f}\".format(name, accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     main.main_slp_linear()\n",
    "#     main.main_slp()\n",
    "#     main.main_mlp()\n",
    "#     main.main_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
