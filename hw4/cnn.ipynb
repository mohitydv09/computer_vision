{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# import main_functions as main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch(im_train, label_train, batch_size):\n",
    "    # TO DO\n",
    "    ## One hot encode the labels.\n",
    "    label_train_encoded = np.eye(10)[label_train[0]].T\n",
    "\n",
    "    ## Get shuffled indices.\n",
    "    np.random.seed(37)  ## 33 gave results for MLP.\n",
    "    random_indices = np.random.permutation(label_train.shape[1])\n",
    "\n",
    "    ## Get the Mini Batches, numpy automatically takes care of the last batch\n",
    "    mini_batch_x = [im_train[:,random_indices[i*batch_size:(i+1)*batch_size]] for i in range(math.ceil(im_train.shape[1]/batch_size))]\n",
    "    mini_batch_y = [label_train_encoded[:,random_indices[i*batch_size:(i+1)*batch_size]] for i in range(math.ceil(im_train.shape[1]/batch_size))]\n",
    "    \n",
    "    return mini_batch_x, mini_batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x, w, b):\n",
    "    # TO DO\n",
    "    y = w@x + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward(dl_dy, x, w, b, y):\n",
    "    # TO DO\n",
    "    dl_dw = dl_dy @ x.T\n",
    "    dl_db = dl_dy\n",
    "    dl_dx = w.T @ dl_dy\n",
    "    \n",
    "    return dl_dx, dl_dw, dl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_euclidean(y_tilde, y):\n",
    "    # TO DO\n",
    "    l = np.sum(np.square(y_tilde - y))\n",
    "    dl_dy = 2*(y_tilde - y) \n",
    "    \n",
    "    return l, dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy_softmax(x, y):\n",
    "    # TO DO\n",
    "    ## Added for numerical stability.\n",
    "    x_max = np.max(x)\n",
    "    ## Apply Softmax layer.\n",
    "    y_telda = np.exp(x-x_max)/(np.sum(np.exp(x-x_max)) + 1e-10)\n",
    "\n",
    "    # Get the softmax loss.\n",
    "    l = -np.sum(y*np.log(y_telda + 1e-10))\n",
    "\n",
    "    dl_dy = y_telda - y\n",
    "\n",
    "    return l, dl_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # TO DO\n",
    "    x[x < 0] = 0\n",
    "    y = x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    dl_dx = np.where(x > 0, dl_dy, 0)\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(image, kernal_size, stride):\n",
    "    # Assumes single channel input image.\n",
    "    num_cols = ((image.shape[0]-kernal_size)//stride +1)*((image.shape[1]-kernal_size)//stride + 1)\n",
    "\n",
    "    col_image = np.empty((kernal_size*kernal_size, num_cols))\n",
    "    col_index = 0\n",
    "    for col in range(0, image.shape[1]-kernal_size+1, stride):\n",
    "        for row in range(0, image.shape[0]-kernal_size+1, stride):\n",
    "            patch = image[row:row+kernal_size, col:col+kernal_size]\n",
    "            col_image[:,col_index] = patch.flatten(order='F')\n",
    "            col_index+=1\n",
    "    return col_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [14 15 16]\n",
      " [28 29 30]]\n",
      "[ 0.  0.  0.  0. 14. 28.  1. 15. 29.]\n"
     ]
    }
   ],
   "source": [
    "image = np.arange(196).reshape(14,14)\n",
    "print(image[0:3,0:3])\n",
    "image = np.pad(image, ((1,1),(1,1)), 'constant')\n",
    "oout = im2col(image, 3, 1)\n",
    "print(oout[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, w_conv, b_conv):\n",
    "    # TO DO\n",
    "\n",
    "    ## Pad the imput image.\n",
    "    x_padded = np.pad(x,((1,1),(1,1),(0,0)),mode='constant')  ## 16,16,1\n",
    "\n",
    "    ## Create the col_image.\n",
    "    kernal_size, stride = 3, 1\n",
    "    col_image = im2col(x_padded.reshape((x_padded.shape[0],x_padded.shape[1])), kernal_size, stride)  ## 9,196 (Channel assumed one here, as the output is 2D)\n",
    "\n",
    "    ## Create flattened weights.\n",
    "    w = np.reshape(w_conv.T, (3,9), order='C')  ## 3,9\n",
    " \n",
    "    ## Perform the convolution.\n",
    "    y = (w @ col_image + b_conv).T            ## 196, 3\n",
    "\n",
    "    ## Reshape to image format.\n",
    "    y = np.reshape(y,(14,14,3), order='F')\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dl_dy, x, w_conv, b_conv, y):\n",
    "    # TO DO\n",
    "    ## Pad the input image.\n",
    "    x_padded = np.pad(x, ((1,1),(1,1),(0,0)), mode='constant')\n",
    "\n",
    "    ## Get the col Image.\n",
    "    x_col = im2col(x_padded.reshape((x_padded.shape[0],x_padded.shape[1])),3,1)        ## 9,196\n",
    "\n",
    "    ## Flatten the dl_dy.\n",
    "    dl_dy_flat = dl_dy.reshape((14*14,3),order='F')    ## 196,3\n",
    "    \n",
    "    dl_dw = (dl_dy_flat.T) @ (x_col.T)                  ## 3,9\n",
    "    dl_dw = (dl_dw.T).reshape(w_conv.shape, order='F')\n",
    "\n",
    "    dl_db = np.sum(dl_dy, axis=(0,1)).reshape(b_conv.shape, order='F')\n",
    "\n",
    "    return dl_dw, dl_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2x2(x):\n",
    "    # TO DO\n",
    "    # print(\"Input shape to pool 2*2 : \" , x.shape)  ## 14,14,3\n",
    "    stride = 2\n",
    "    y = np.zeros((x.shape[0]//2, x.shape[1]//2, x.shape[2]))\n",
    "    for channel in range(x.shape[2]):\n",
    "        for col in range(0,x.shape[1]-1,stride):\n",
    "            for row in range(0,x.shape[0]-1,stride):\n",
    "                y[row//2, col//2, channel] = np.max(x[row:row+2, col:col+2, channel])\n",
    "    # print(\"Output of pooling 2*2 : \", y.shape)   ## 7,7,3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2x2_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    stride = 2\n",
    "    dl_dx = np.zeros_like(x)\n",
    "    for channel in range(x.shape[2]):\n",
    "        for col in range(0,x.shape[1],stride):\n",
    "            for row in range(0,x.shape[0],stride):\n",
    "                patch = x[row:row+2, col:col+2,channel]\n",
    "                idx = np.argmax(patch)\n",
    "                max_row, max_col = np.unravel_index(idx, patch.shape)\n",
    "                dl_dx[row+max_row,col+max_col,channel] = dl_dy[row//2,col//2,channel]\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening(x):\n",
    "    # TO DO\n",
    "    y = x.flatten(order='F').reshape((-1,1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening_backward(dl_dy, x, y):\n",
    "    # TO DO\n",
    "    dl_dx = dl_dy.reshape(x.shape, order='F')\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_slp_linear(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 0.01\n",
    "    decay_rate = 0.5\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    w = np.random.randn(output_size,input_size)\n",
    "    b = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw_batch = np.zeros_like(w)\n",
    "        dl_db_batch = np.zeros_like(b)\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            y_tilde = fc(x, w, b)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy = loss_euclidean(y_tilde, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients.\n",
    "            dl_dx, dl_dw, dl_db = fc_backward(dl_dy, x, w, b, y)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_batch += dl_dw\n",
    "            dl_db_batch += dl_db\n",
    "            loss_cum += loss\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w -= (learning_rate/len(mini_batch_x[0]))*dl_dw_batch\n",
    "        b -= (learning_rate/len(mini_batch_x[0]))*dl_db_batch\n",
    "    plt.plot(np.arange(len(losses[10:])), losses[10:])\n",
    "    plt.show()\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_slp(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 5\n",
    "    decay_rate = 0.7\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    w = np.random.randn(output_size,input_size)\n",
    "    b = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw_batch = np.zeros_like(w)\n",
    "        dl_db_batch = np.zeros_like(b)\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            y_tilde = fc(x, w, b)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy = loss_cross_entropy_softmax(y_tilde, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients.\n",
    "            dl_dx, dl_dw, dl_db = fc_backward(dl_dy, x, w, b, y)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_batch += dl_dw\n",
    "            dl_db_batch += dl_db\n",
    "            loss_cum += loss\n",
    "        #     break\n",
    "        # break\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w -= (learning_rate/len(mini_batch_x[0]))*dl_dw_batch\n",
    "        b -= (learning_rate/len(mini_batch_x[0]))*dl_db_batch\n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 5\n",
    "    decay_rate = 0.99\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    input_size = mini_batch_x[0].shape[0]\n",
    "    output_size = mini_batch_y[0].shape[0]\n",
    "    hidden_size = 30\n",
    "    w1 = np.random.randn(hidden_size,input_size)\n",
    "    b1 = np.random.randn(hidden_size,1)\n",
    "    w2 = np.random.randn(output_size,hidden_size)\n",
    "    b2 = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    for iter in range(10001):\n",
    "        if iter%1000 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            learning_rate = learning_rate*decay_rate\n",
    "        dl_dw1_batch = np.zeros_like(w1)\n",
    "        dl_db1_batch = np.zeros_like(b1)\n",
    "        dl_dw2_batch = np.zeros_like(w2)\n",
    "        dl_db2_batch = np.zeros_like(b2)\n",
    "\n",
    "        loss_cum = 0\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass hidden Layer.\n",
    "            y_out_hidden = fc(x, w1, b1)   ## Output will be a 30 size column matrix\n",
    "            ## Apply ReLU on this.\n",
    "            y_out_relu = relu(y_out_hidden)\n",
    "            ## Forward pass output layer\n",
    "            y_out_last = fc(y_out_relu, w2, b2)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy_last = loss_cross_entropy_softmax(y_out_last, y)   ## dl_dy is columns\n",
    "\n",
    "            ## Compute Gradients last layer.\n",
    "            dl_dx_hidden, dl_dw2, dl_db2 = fc_backward(dl_dy_last, y_out_hidden, w2, b2, y_out_last)\n",
    "            ## Computer Gradients ReLU.\n",
    "            dl_dx_relu = relu_backward(dl_dx_hidden, y_out_hidden, y_out_relu)\n",
    "            ## Compute Gradients First Layer.\n",
    "            dl_dx, dl_dw1, dl_db1 = fc_backward(dl_dx_relu, x, w1, b1, y_out_hidden)\n",
    "\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw2_batch += dl_dw2\n",
    "            dl_db2_batch += dl_db2\n",
    "            dl_dw1_batch += dl_dw1\n",
    "            dl_db1_batch += dl_db1\n",
    "            loss_cum += loss\n",
    "        #     break\n",
    "        # break\n",
    "\n",
    "        losses.append(loss_cum)\n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w2 -= (learning_rate/len(mini_batch_x[0]))*dl_dw2_batch\n",
    "        b2 -= (learning_rate/len(mini_batch_x[0]))*dl_db2_batch\n",
    "        w1 -= (learning_rate/len(mini_batch_x[0]))*dl_dw1_batch\n",
    "        b1 -= (learning_rate/len(mini_batch_x[0]))*dl_db1_batch   \n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.show()\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(mini_batch_x, mini_batch_y):\n",
    "    # TO DO\n",
    "    learning_rate = 1.5\n",
    "    decay_rate = 0.98\n",
    "\n",
    "    ## Initialize Weights.\n",
    "    kernal_size = 3\n",
    "    channels_input = 1\n",
    "    channels_output = 3\n",
    "    flattened_size = 147\n",
    "    output_size = 10\n",
    "    w_conv = np.random.randn(kernal_size, kernal_size, channels_input, channels_output)\n",
    "    b_conv = np.random.randn(channels_output,1)\n",
    "    w_fc = np.random.randn(output_size,flattened_size)\n",
    "    b_fc = np.random.randn(output_size,1)\n",
    "\n",
    "    k = 0\n",
    "    losses = []\n",
    "    loss_cum = 0\n",
    "    for iter in range(5001):\n",
    "        # print(\"iter no : \", iter)\n",
    "        if iter%100 == 0:\n",
    "            print(\"Iteration Number is : \", iter, \" Loss is : \", losses[-1] if len(losses) > 0 else 0)\n",
    "            # learning_rate = learning_rate*decay_rate\n",
    "            print(\"Learning Rate : \", learning_rate)\n",
    "        if (iter+1)%2000 == 0:\n",
    "            learning_rate = learning_rate*0.8\n",
    "\n",
    "        dl_dw_conv_batch = np.zeros_like(w_conv)\n",
    "        dl_db_conv_batch = np.zeros_like(b_conv)\n",
    "        dl_dw_fc_batch = np.zeros_like(w_fc)\n",
    "        dl_db_fc_batch = np.zeros_like(b_fc)\n",
    "\n",
    "\n",
    "        for x, y in zip(mini_batch_x[k].T, mini_batch_y[k].T):\n",
    "            x = x.reshape((14,14,1), order=\"F\") ## Revert back to being a image.\n",
    "            y = y.reshape(-1,1)\n",
    "\n",
    "            ## Forward pass.\n",
    "            # print(\"B Conv : \", b_conv.flatten())\n",
    "            y_conv = conv(x, w_conv, b_conv)   ## Output should be  : (14, 14, 3)\n",
    "            # print(\"Y Conv : \", y_conv)\n",
    "            y_relu = relu(y_conv)                                   # (14, 14, 3)\n",
    "            # print(\"Y ReLU : \", y_relu)\n",
    "            y_maxpool = pool2x2(y_relu)                             # (7, 7, 3)\n",
    "            # print('y max pool : ', y_maxpool)                             \n",
    "            y_flatten = flattening(y_maxpool)                       # (147, 1)\n",
    "            # print(\"Y Flatten : \", y_flatten)\n",
    "            y_fc = fc(y_flatten, w_fc, b_fc)                        # (10, 1)\n",
    "            # print(\"Y FC : \", y_fc)\n",
    "\n",
    "            ## Compute Loss.\n",
    "            loss, dl_dy_softmax = loss_cross_entropy_softmax(y_fc, y)   ## dl_dy is columns\n",
    "            # print(\"Loss : \", loss)\n",
    "            # print( \"dl_dy_softmax : \", dl_dy_softmax)\n",
    "\n",
    "            ## Compute Gradients in reverse.\n",
    "            dl_dx_flatten, dl_dw_fc, dl_db_fc = fc_backward(dl_dy_softmax, y_flatten, w_fc, b_fc, y_fc) \n",
    "            # print(\"dl_dw_fc : \", dl_dw_fc)\n",
    "            # print(\"dl_db_fc : \", dl_db_fc)\n",
    "            # print(\"dl_dx_flatten : \", dl_dx_flatten)\n",
    "            dl_dx_maxpool = flattening_backward(dl_dx_flatten, y_maxpool, y_flatten)\n",
    "            # print(\"dl_dx_maxpool : \", dl_dx_maxpool)\n",
    "            dl_dx_relu = pool2x2_backward(dl_dx_maxpool, y_relu, y_maxpool)\n",
    "            # print(\"dl_dx_relu : \", dl_dx_relu)\n",
    "            dl_dx_conv = relu_backward(dl_dx_relu, y_relu, y_maxpool)\n",
    "            # print(\"dl_dx_conv : \", dl_dx_conv)\n",
    "            dl_dw_conv, dl_db_conv = conv_backward(dl_dx_conv, x, w_conv, b_conv, y_conv)\n",
    "            # print(\"dl_dw_conv : \", dl_dw_conv)\n",
    "            # print(\"DL_BD Inn backward : \", dl_db_conv.flatten())\n",
    "\n",
    "            # print(\"dl_dw : \", dl_dw_conv)\n",
    "            ## Add to the gradients of batch.\n",
    "            dl_dw_conv_batch += dl_dw_conv\n",
    "            dl_db_conv_batch += dl_db_conv\n",
    "            dl_dw_fc_batch += dl_dw_fc\n",
    "            dl_db_fc_batch += dl_db_fc\n",
    "            loss_cum += loss\n",
    "            # break\n",
    "        # break\n",
    "        # print(\"B Conv : \", b_conv)\n",
    "        # print(\"dl_db_conv_batch : \", dl_db_conv_batch)\n",
    "        \n",
    "        k += 1\n",
    "        if k == len(mini_batch_x):\n",
    "            losses.append(loss_cum)\n",
    "            loss_cum = 0\n",
    "            k = 0\n",
    "        ## Update weights\n",
    "        w_conv -= (learning_rate/len(mini_batch_x[0]))*dl_dw_conv_batch\n",
    "        b_conv -= (learning_rate/len(mini_batch_x[0]))*dl_db_conv_batch\n",
    "        w_fc -= (learning_rate/len(mini_batch_x[0]))*dl_dw_fc_batch\n",
    "        b_fc -= (learning_rate/len(mini_batch_x[0]))*dl_db_fc_batch   \n",
    "    plt.plot(np.arange(len(losses[5:])), losses[5:])\n",
    "    plt.show()\n",
    "    return w_conv, b_conv, w_fc, b_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number is :  0  Loss is :  0\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  100  Loss is :  0\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  200  Loss is :  0\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  300  Loss is :  0\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  400  Loss is :  28554.17358526943\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  500  Loss is :  28554.17358526943\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  600  Loss is :  28554.17358526943\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  700  Loss is :  28554.17358526943\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  800  Loss is :  27624.043133367126\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  900  Loss is :  27624.043133367126\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1000  Loss is :  27624.043133367126\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1100  Loss is :  27624.043133367126\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1200  Loss is :  27553.540435199087\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1300  Loss is :  27553.540435199087\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1400  Loss is :  27553.540435199087\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1500  Loss is :  27195.290392995197\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1600  Loss is :  27195.290392995197\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1700  Loss is :  27195.290392995197\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1800  Loss is :  27195.290392995197\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  1900  Loss is :  22635.92360056621\n",
      "Learning Rate :  1.5\n",
      "Iteration Number is :  2000  Loss is :  22635.92360056621\n",
      "Learning Rate :  1.2000000000000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # main_slp_linear()\n",
    "    # main_slp()\n",
    "    # main_mlp()\n",
    "    main_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from cnn import get_mini_batch, fc, relu, conv, pool2x2, flattening\n",
    "# from cnn import train_slp_linear, train_slp, train_mlp, train_cnn\n",
    "\n",
    "\n",
    "def main_slp_linear():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w, b = train_slp_linear(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('slp_linear.mat', mdict={'w': w, 'b': b})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        y = fc(x, w, b)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Single-layer Linear Perceptron Confusion Matrix')\n",
    "\n",
    "def main_slp():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w, b = train_slp(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('slp.mat', mdict={'w': w, 'b': b})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        y = fc(x, w, b)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Single-layer Perceptron Confusion Matrix')\n",
    "\n",
    "def main_mlp():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w1, b1, w2, b2 = train_mlp(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('mlp.mat', mdict={'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2})\n",
    "\n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]]\n",
    "        pred1 = fc(x, w1, b1)\n",
    "        pred2 = relu(pred1)\n",
    "        y = fc(pred2, w2, b2)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'Multi-layer Perceptron Confusion Matrix')\n",
    "\n",
    "def main_cnn():\n",
    "    mnist_train = sio.loadmat('./mnist_train.mat')\n",
    "    mnist_test = sio.loadmat('./mnist_test.mat')\n",
    "    im_train, label_train = mnist_train['im_train'], mnist_train['label_train']\n",
    "    im_test, label_test = mnist_test['im_test'], mnist_test['label_test']\n",
    "    batch_size = 32\n",
    "    im_train, im_test = im_train / 255.0, im_test / 255.0\n",
    "    mini_batch_x, mini_batch_y = get_mini_batch(im_train, label_train, batch_size)\n",
    "    w_conv, b_conv, w_fc, b_fc = train_cnn(mini_batch_x, mini_batch_y)\n",
    "    sio.savemat('cnn.mat', mdict={'w_conv': w_conv, 'b_conv': b_conv, 'w_fc': w_fc, 'b_fc': b_fc})\n",
    "    # could use following two lines to replace above two lines if only want to check results\n",
    "    # data = sio.loadmat('cnn.mat')\n",
    "    # w_conv, b_conv, w_fc, b_fc = data['w_conv'], data['b_conv'], data['w_fc'], data['b_fc']\n",
    "    \n",
    "    acc = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    num_test = im_test.shape[1]\n",
    "    for i in range(num_test):\n",
    "        x = im_test[:, [i]].reshape((14, 14, 1), order='F')\n",
    "        pred1 = conv(x, w_conv, b_conv)  # (14, 14, 3)\n",
    "        pred2 = relu(pred1)  # (14, 14, 3)\n",
    "        pred3 = pool2x2(pred2)  # (7, 7, 3)\n",
    "        pred4 = flattening(pred3)  # (147, 1)\n",
    "        y = fc(pred4, w_fc, b_fc)  # (10, 1)\n",
    "        l_pred = np.argmax(y)\n",
    "        confusion[l_pred, label_test[0, i]] = confusion[l_pred, label_test[0, i]] + 1\n",
    "        if l_pred == label_test[0, i]:\n",
    "            acc = acc + 1\n",
    "    accuracy = acc / num_test\n",
    "    for i in range(10):\n",
    "        confusion[:, i] = confusion[:, i] / np.sum(confusion[:, i])\n",
    "\n",
    "    label_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes, 'CNN Confusion Matrix')\n",
    "\n",
    "def visualize_confusion_matrix(confusion, accuracy, label_classes, name):\n",
    "    plt.title(\"{}, accuracy = {:.3f}\".format(name, accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     main.main_slp_linear()\n",
    "#     main.main_slp()\n",
    "#     main.main_mlp()\n",
    "#     main.main_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
