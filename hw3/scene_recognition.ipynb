{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from pathlib import Path, PureWindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset_info(data_path):\n",
    "    # extract information from train.txt\n",
    "    f = open(os.path.join(data_path, \"train.txt\"), \"r\")\n",
    "    contents_train = f.readlines()\n",
    "    label_classes, label_train_list, img_train_list = [], [], []\n",
    "    for sample in contents_train:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        if label not in label_classes:\n",
    "            label_classes.append(label)\n",
    "        label_train_list.append(sample[0])\n",
    "        img_train_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))\n",
    "    print('Classes: {}'.format(label_classes))\n",
    "\n",
    "    # extract information from test.txt\n",
    "    f = open(os.path.join(data_path, \"test.txt\"), \"r\")\n",
    "    contents_test = f.readlines()\n",
    "    label_test_list, img_test_list = [], []\n",
    "    for sample in contents_test:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        label_test_list.append(label)\n",
    "        img_test_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))  # you can directly use img_path if you run in Windows\n",
    "\n",
    "    return label_classes, label_train_list, img_train_list, label_test_list, img_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 128)\n"
     ]
    }
   ],
   "source": [
    "def compute_dsift(img):\n",
    "    # To do\n",
    "    # Create a sift object.\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Stride and Size.\n",
    "    stride = 200\n",
    "    size = 8\n",
    "\n",
    "    # print(\"Shape of image : \", img.shape)\n",
    "    u = [i for i in range(0, img.shape[1]-size+1,stride)]   # Plus one was done to consider the last line\n",
    "    v = [i for i in range(0, img.shape[0]-size+1, stride)]\n",
    "    X, Y = np.meshgrid(u,v)\n",
    "    X = (X.ravel() + size//2).astype(float)\n",
    "    Y = (Y.ravel() + size//2).astype(float)\n",
    "    # print(X)\n",
    "    # print(Y)\n",
    "\n",
    "    # Make Keypoints where you want to calculate the Descriptor.\n",
    "    keypoints = [cv2.KeyPoint(x, y, size) for (x, y) in zip(X , Y)]\n",
    "\n",
    "    # Make the descriptors for all the keypoints locations.\n",
    "    keypoints, dense_feature = sift.compute(img, keypoints)\n",
    "\n",
    "    return dense_feature\n",
    "\n",
    "img = cv2.imread(\"image_0043.jpg\",0)\n",
    "dense_features = compute_dsift(img)\n",
    "print(dense_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_tiny_image(img, output_size):\n",
    "    # To do\n",
    "    # img is gray scale image. numpy array.\n",
    "    # output size will be a tuple of size w,h.\n",
    "\n",
    "    hor_window_size = img.shape[1] // output_size[1]\n",
    "    vert_window_size = img.shape[0] // output_size[0]\n",
    "    feature = np.zeros((output_size))\n",
    "    for row in range(output_size[0]):\n",
    "        for col in range(output_size[1]):\n",
    "            # print(\"Item here : \\n\", img[row*vert_window_size :(row+1)*vert_window_size , col*hor_window_size : (col+1)*hor_window_size] )\n",
    "            feature[row,col] = np.mean(img[row*vert_window_size :(row+1)*vert_window_size , col*hor_window_size : (col+1)*hor_window_size])\n",
    "\n",
    "    # feature = feature/(hor_window_size*vert_window_size)\n",
    "    # Normalize the image.\n",
    "\n",
    "    # print(\"Feature without Norm : \", feature)\n",
    "    # print(\"mean of features  : \", np.mean(feature))\n",
    "    # print(\"Max of features : \", np.max(feature))\n",
    "    # print(\"Min of features : \", np.min(feature))\n",
    "    # feature = (feature - np.mean(feature)) / np.std(feature)\n",
    "    # feature = (feature - np.mean(feature)) / np.max(feature)\n",
    "    feature = (feature - np.mean(feature)) / 255\n",
    "    # print(\"Normalization is incomplete check afterwards.\")\n",
    "    # print(\"Feature matrix normalized : \", feature)\n",
    "\n",
    "    return feature\n",
    "\n",
    "img = cv2.imread(\"image_0043.jpg\",0)\n",
    "# plt.imshow(img, cmap='grey')\n",
    "output_size = (16,16)\n",
    "tiny_image = get_tiny_image(img, output_size)\n",
    "fig = plt.figure(figsize=(2.5,2.5))\n",
    "plt.imshow(tiny_image, cmap='grey')\n",
    "\n",
    "# Trial array.\n",
    "flat = np.array([i for i in range(256)])\n",
    "array = flat.reshape((16,16))\n",
    "# plt.imshow(array,cmap='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(feature_train, label_train, feature_test, k):\n",
    "    # To do\n",
    "\n",
    "    # Initialize kNN.\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(feature_train,label_train)\n",
    "\n",
    "    dist, indices = neigh.kneighbors(feature_test)\n",
    "    # print(\"Shape of predict KNN : \", indices.shape)\n",
    "    # print(\"Indices in predict knn : \", indices)\n",
    "    \n",
    "    # label_test_pred = label_train[indices]\n",
    "    label_test_pred = [label_train[i] for i in indices[:,0]]\n",
    "    \n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m     visualize_confusion_matrix(confusion, accuracy, label_classes)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m confusion, accuracy\n\u001b[0;32m---> 56\u001b[0m classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
      "Cell \u001b[0;32mIn[87], line 16\u001b[0m, in \u001b[0;36mclassify_knn_tiny\u001b[0;34m(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m feature_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(img_train_list), output_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39moutput_size[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,path_img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(img_train_list):\n\u001b[0;32m---> 16\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(path_img,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Read Image, 0 to read as greyscale.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     feature_train[i,:] \u001b[38;5;241m=\u001b[39m get_tiny_image(img, output_size)\u001b[38;5;241m.\u001b[39mflatten()   \u001b[38;5;66;03m# Get features using tiny image funtion.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the tiny image of all the test items.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "\n",
    "    output_size = (16,16)\n",
    "\n",
    "    # Map labels to integers.\n",
    "    label_map = {key : value+1 for value, key in enumerate(label_classes)}\n",
    "    label_train_int = np.array([label_map[item] for item in label_train_list])\n",
    "    label_test_int = np.array([label_map[item] for item in label_test_list])\n",
    "\n",
    "    label_classes_int = np.array([label_map[item] for item in label_classes])\n",
    "\n",
    "    # Get the tiny image of all the train items.\n",
    "    feature_train = np.zeros((len(img_train_list), output_size[0]*output_size[1]))\n",
    "    for i,path_img in enumerate(img_train_list):\n",
    "        img = cv2.imread(path_img,0) # Read Image, 0 to read as greyscale.\n",
    "        feature_train[i,:] = get_tiny_image(img, output_size).flatten()   # Get features using tiny image funtion.\n",
    "\n",
    "    # Get the tiny image of all the test items.\n",
    "    feature_test = np.zeros((len(img_test_list), output_size[0]*output_size[1]))\n",
    "    for i,path_img in enumerate(img_test_list):\n",
    "        img = cv2.imread(path_img,0) # 0 to read as greyscale.\n",
    "        feature_test[i,:] = get_tiny_image(img, output_size).flatten()\n",
    "    \n",
    "    # Predict labels for the test images.\n",
    "    k_neighbour = 1\n",
    "    label_test_pred = predict_knn(feature_train, label_test_int, feature_test, k_neighbour)\n",
    "\n",
    "    print(\"Label test Pred : \", label_test_pred)\n",
    "\n",
    "    # Make the confusion Matrix.\n",
    "    confusion = np.zeros((len(label_classes), len(label_classes)))\n",
    "    for row,cls in enumerate(label_classes_int):\n",
    "        # Get the indices where test data has this item.(ground truth)\n",
    "        # print(\"Row is : \", row, \" Cls is : \", cls)\n",
    "        indices = np.where(label_test_int == cls)[0]\n",
    "        # print(\"Len of indices in the loop  : \", len(indices))\n",
    "        # Get the predictions for this class of data.\n",
    "        preds_temp = np.array([label_test_pred[i] for i in indices])\n",
    "        # print('preds temps : ', preds_temp)\n",
    "        for col in range(len(label_classes)):\n",
    "            # True label for this is cls.\n",
    "            # confusion[row,col] = np.sum(np.where(preds_temp == label_classes_int[col])) / len(indices)\n",
    "            confusion[row,col] = np.sum(preds_temp == label_classes_int[col]) / len(indices)\n",
    "            # print(\"Confusion Value is : \", confusion[row,col])\n",
    "\n",
    "    # confusion[0,14] = 1\n",
    "    print(\"Max in confution matrix : \", np.max(confusion))\n",
    "    # Find the accuracy as mean of the correct predictions.\n",
    "    accuracy = np.mean(np.diagonal(confusion))\n",
    "\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "\n",
    "    return confusion, accuracy\n",
    "\n",
    "classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape and Size of Dense Features :  (5956, 128)\n"
     ]
    }
   ],
   "source": [
    "def build_visual_dictionary(dense_feature_list, dict_size):\n",
    "    # To do\n",
    "\n",
    "    # For each Image, compute dense SIFT over regular grid.\n",
    "\n",
    "    ## Assume that dense_feature_list is list of training images.\n",
    "    # Get the dense features image of all the train items.\n",
    "    dense_features = np.empty((0,128))\n",
    "    for i,path_img in enumerate(dense_feature_list):\n",
    "        img = cv2.imread(path_img,0) # Read Image, 0 to read as greyscale.\n",
    "        features = compute_dsift(img)\n",
    "        dense_features = np.vstack((dense_features, features))\n",
    "        \n",
    "    print(\"Shape and Size of Dense Features : \", dense_features.shape)\n",
    "    \n",
    "    # Initialize a kmean object.\n",
    "    kmeans = KMeans(n_clusters=dict_size, init='random', n_init=10 ,max_iter=300)\n",
    "    # Fit the data into kmeans.\n",
    "    kmeans.fit(dense_features)\n",
    "\n",
    "    vocab = kmeans.cluster_centers_\n",
    "    return vocab\n",
    "\n",
    "dict_size = 50\n",
    "vocab = build_visual_dictionary(img_train_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(feature, vocab):\n",
    "    # To do\n",
    "\n",
    "    # Find the nearest word to item from feature to vocab.\n",
    "    # Initailize a nearest neighbour object.\n",
    "    neigh = NearestNeighbors(n_neighbors=1)\n",
    "    neigh.fit(vocab)\n",
    "    distance, indices = neigh.kneighbors(feature)\n",
    "\n",
    "    # bow_feature = [np.sum(indices == i) for i in range(vocab.shape[0])]\n",
    "    bow_feature = np.bincount(indices[:,0], minlength=vocab.shape[0])\n",
    "    # print(\"Bow feature : \", bow_feature)\n",
    "    # print(\"Len of bow : \" , len(bow_feature))\n",
    "\n",
    "    # Normalize the bow_features.\n",
    "    bow_feature = bow_feature / np.linalg.norm(bow_feature)\n",
    "\n",
    "    # print(bow_feature)\n",
    "\n",
    "    return bow_feature\n",
    "\n",
    "img = cv2.imread(\"image_0043.jpg\",0)\n",
    "feature = compute_dsift(img)\n",
    "bow_feature = compute_bow(feature, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm(feature_train, label_train, feature_test, n_classes):\n",
    "    # To do\n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(confusion, accuracy, label_classes):\n",
    "    plt.title(\"accuracy = {:.3f}\".format(accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Kitchen', 'Store', 'Bedroom', 'LivingRoom', 'Office', 'Industrial', 'Suburb', 'InsideCity', 'TallBuilding', 'Street', 'Highway', 'OpenCountry', 'Coast', 'Mountain', 'Forest']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # To do: replace with your dataset path\n",
    "    label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "    \n",
    "    # classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "\n",
    "    # classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n",
    "    # classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_classes))\n",
    "type(label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_train_list))\n",
    "print(label_train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_test_list)\n",
    "label_test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
